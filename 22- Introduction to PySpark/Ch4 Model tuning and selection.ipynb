{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081754dd",
   "metadata": {},
   "source": [
    "## What is logistic regression?\n",
    "\n",
    "The model you'll be fitting in this chapter is called a logistic regression. \n",
    "<u>This model is very similar to a linear regression, but instead of predicting a numeric variable, it predicts the probability (between 0 and 1) of an event.</u>\n",
    "\n",
    "To use this as a classification algorithm, all you have to do is assign a cutoff point to these probabilities. If the predicted probability is above the cutoff point, you classify that observation as a 'yes' (in this case, the flight being late), if it's below, you classify it as a 'no'!\n",
    "\n",
    "You'll tune this model by testing different values for several hyperparameters. A hyperparameter is just a value in the model that's not estimated from the data, but rather is supplied by the user to maximize performance. For this course it's not necessary to understand the mathematics behind all of these values - what's important is that you'll try out a few different choices and pick the best one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd2dbe",
   "metadata": {},
   "source": [
    "## Create the modeler\n",
    "\n",
    "The Estimator you'll be using is a `LogisticRegression` from the `pyspark.ml.classification` submodule.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the `LogisticRegression` class from `pyspark.ml.classification.`\n",
    "- Create a LogisticRegression called lr by calling `LogisticRegression()` with no arguments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f81671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a LogisticRegression Estimator\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd709ac",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "In the next few exercises you'll be tuning your logistic regression model using a procedure called `k-fold cross validation.` This is a method of estimating the model's performance on unseen data (like your test DataFrame).\n",
    "\n",
    "<u>It works by splitting the training data into a few different partitions. The exact number is up to you, but in this course you'll be using PySpark's default value of three. Once the data is split up, one of the partitions is set aside, and the model is fit to the others. Then the error is measured against the held out partition. This is repeated for each of the partitions, so that every block of data is held out and used as a test set exactly once. Then the error on each of the partitions is averaged. This is called the cross validation error of the model, and is a good estimate of the actual error on the held out data.</u>\n",
    "\n",
    "You'll be using cross validation to choose the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, `elasticNetParam` and `regParam`, and using the cross validation error to compare all the different models so you can choose the best one!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9445b7a3",
   "metadata": {},
   "source": [
    "## Create the evaluator\n",
    "\n",
    "The first thing you need when doing cross validation for model selection is a way to compare different models. Luckily, the `pyspark.ml.evaluation` submodule has classes for evaluating different kinds of models. Your model is a binary classification model, so you'll be using the `BinaryClassificationEvaluator` from the `pyspark.ml.evaluation`module.\n",
    "\n",
    "This evaluator calculates the area under the ROC. This is a metric that combines the two kinds of errors a binary classifier can make (false positives and false negatives) into a simple number. You'll learn more about this towards the end of the chapter!\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the submodule `pyspark.ml.evaluation` as evals.\n",
    "- Create `evaluator` by calling `evals.BinaryClassificationEvaluator()` with the argument `metricName=\"areaUnderROC\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluation submodule\n",
    "import pyspark.ml.evaluation as evals\n",
    "\n",
    "# Create a BinaryClassificationEvaluator\n",
    "evaluator = evals.BinaryClassificationEvaluator(metricName='areaUnderROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9ff48",
   "metadata": {},
   "source": [
    "## Make a grid\n",
    "\n",
    "Create a grid of values to search over when looking for the optimal hyperparameters. The submodule `pyspark.ml.tuning `includes a class called `ParamGridBuilder` that does just that (maybe you're starting to notice a pattern here; PySpark has a submodule for just about everything!).\n",
    "\n",
    "You'll need to use the `.addGrid()` and `.build()` methods to create a grid that you can use for cross validation. The `.addGrid()` method takes a model parameter (an attribute of the model `Estimator`, `lr`, that you created a few exercises ago) and a list of values that you want to try. The .`build()` method takes no arguments, it just returns the grid that you'll use later.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Import the submodule `pyspark.ml.tuning` under the alias `tune`.\n",
    "- Call the class constructor `ParamGridBuilder()` with no arguments. Save this as `grid`.\n",
    "- Call the .`addGrid()` method on grid with `lr.regParam` as the first argument and `np.arange(0, .1, .01)` as the second argument. This second call is a function from the numpy module (imported as np) that creates a list of numbers from 0 to .1, incrementing by .01. Overwrite `grid` with the result.\n",
    "- Update grid again by calling the `addGrid()` method a second time create a grid for `lr.elasticNetParam` that includes only the values `[0, 1].`\n",
    "- Call the `.build()` method on grid and overwrite it with the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b188e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tuning submodule\n",
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "# Create the parameter grid\n",
    "grid = tune.ParamGridBuilder()\n",
    "\n",
    "# Add the hyperparameter\n",
    "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
    "grid = grid.addGrid(lr.elasticNetParam, [0,1])\n",
    "\n",
    "# Build the grid\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92301e80",
   "metadata": {},
   "source": [
    "## Make the validator\n",
    "\n",
    "The submodule `pyspark.ml.tuning` also has a class called `CrossValidator` for performing cross validation. This `Estimator` takes the modeler you want to fit, the grid of hyperparameters you created, and the evaluator you want to use to compare your models.\n",
    "\n",
    "The submodule `pyspark.ml.tune` has already been imported as `tune.` You'll create the `CrossValidator` by passing it the logistic regression `Estimator` `lr`, the parameter `grid`, and the `evaluator` you created in the previous exercises.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create a CrossValidator by calling tune.CrossValidator() with the arguments:\n",
    "    - estimator=lr\n",
    "    - estimatorParamMaps=grid\n",
    "    - evaluator=evaluator\n",
    "- Name this object cv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CrossValidator\n",
    "cv = tune.CrossValidator(estimator=lr,\n",
    "               estimatorParamMaps=grid,\n",
    "               evaluator=evaluator\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23831f84",
   "metadata": {},
   "source": [
    "## Fit the model(s)\n",
    "\n",
    "You're finally ready to fit the models and select the best one!\n",
    "\n",
    "Unfortunately, cross validation is a very computationally intensive procedure. Fitting all the models would take too long on DataCamp.\n",
    "\n",
    "To do this locally you would use the code:\n",
    "> # Fit cross validation models\n",
    "> models = cv.fit(training)\n",
    "> # Extract the best model\n",
    "> best_lr = models.bestModel\n",
    "\n",
    "Remember, the training data is called training and you're using lr to fit a logistic regression model. Cross validation selected the parameter values `regParam=0` and `elasticNetParam=0` as being the best. These are the default values, so you don't need to do anything else with lr before fitting the model.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Create `best_lr` by calling `lr.fit()` on the training data.\n",
    "- Print `best_lr` to verify that it's an object of the `LogisticRegressionModel` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90a696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call lr.fit()\n",
    "best_lr = lr.fit(training)\n",
    "\n",
    "# Print best_lr\n",
    "print(best_lr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5ffa1",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Remember the test data that you set aside waaaaaay back in chapter 3? It's finally time to test your model on it! You can use the same evaluator you made to fit the model.\n",
    "\n",
    "**Instructions**\n",
    "\n",
    "- Use your model to generate predictions by applying best_lr.transform() to the test data. Save this as test_results.\n",
    "- Call evaluator.evaluate() on test_results to compute the AUC. Print the output.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2801c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict the test set\n",
    "test_results = best_lr.transform(test)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print(evaluator.evaluate(test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91bdc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee253c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41372f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ce3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
