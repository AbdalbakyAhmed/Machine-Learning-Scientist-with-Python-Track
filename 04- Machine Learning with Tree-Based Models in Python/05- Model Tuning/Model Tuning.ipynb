{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c3313d",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "**The hyperparameters of a machine learning model are parameters that are not learned from data. They should be set prior to fitting the model to the training set to tune the hyperparameters of a tree-based model using grid search cross validation.**\n",
    "\n",
    "#### Tuning a CART's hyperparameters\n",
    "\n",
    "#### Machine learning model:\n",
    "\n",
    "* parameters: learned from data\n",
    "    * CART example: split-point of a node, split- feature of a node, ...\n",
    "* hyperparameters: not learned from data, set prior to training\n",
    "    * CART example: max_depth, min_samples_leaf, splitting criterion...\n",
    "\n",
    "#### What is hyperparameter tuning?\n",
    "\n",
    "* Problem: search for a set of optimal hyperparameters for a learning algorithm.\n",
    "* Solution: find a set of optimal hyperparameters that results in an optimal model.\n",
    "* Optimal model: yields an optimal score.\n",
    "* Score: in sklearn defaults to accuracy (classification) and R (regression).\n",
    "* Cross validation is used to estimate the generalization performance.\n",
    "\n",
    "#### Why tune hyperparameters?\n",
    "\n",
    "* In sklearn, a model's default hyperparameters are not optimal for all problems.\n",
    "* Hyperparameters should be tuned to obtain the best model performance.\n",
    "\n",
    "#### Grid search cross validation\n",
    "\n",
    "* Manually set a grid of discrete hyperparameter values.\n",
    "\n",
    "* Set a metric for scoring model performance.\n",
    "\n",
    "* hyperparameter space = \n",
    "\n",
    "Search exhaustively through the grid.\n",
    "For each set of hyperparameters, evaluate each model's CV score.\n",
    "\n",
    "* CV scores = { \n",
    "\n",
    "        , . . . }\n",
    "* The optimal hyperparameters are those of the model achieving the best CV score.\n",
    "    * optimal hyperparameters = set of hyperparameters corresponding to the best CV score\n",
    "\n",
    "#### Example:\n",
    "\n",
    "#### Hyperparameter of DecisionTreeClassifier\n",
    "\n",
    "* DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
    "            splitter='best')\n",
    "\n",
    "=> mean_features not a Hyperparameter of DecisionTreeClassifier\n",
    "#### Doing\n",
    "\n",
    "* Inspecting the hyperparameters of a CART in sklearn\n",
    "* Extracting the best hyperparameters\n",
    "    * Set the tree's hyperparameter grid\n",
    "    * Define params_dt\n",
    "    * Evaluate the optimal tree\n",
    "    * Compute the test set ROC AUC score.\n",
    "* Extracting the best estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7082328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "SEED =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9dfec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "liver = pd.read_csv('../indian_liver_patient/indian_liver_patient_preprocessed.csv', index_col = 0)\n",
    "X = liver.drop('Liver_disease', axis = 1)\n",
    "y = liver['Liver_disease']\n",
    "\n",
    "dt = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, random_state=1,\n",
    "            splitter='best')\n",
    "\n",
    "\n",
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e8361",
   "metadata": {},
   "source": [
    "## Set the tree's hyperparameter grid\n",
    "\n",
    "In this exercise, you'll manually set the grid of hyperparameters that will be used to tune the classification tree dt and find the optimal classifier in the next exercise.\n",
    "**Instructions**\n",
    "\n",
    "* Define a grid of hyperparameters corresponding to a Python dictionary called params_dt with:\n",
    "\n",
    "    * the key 'max_depth' set to a list of values 2, 3, and 4\n",
    "\n",
    "    * the key 'min_samples_leaf' set to a list of values 0.12, 0.14, 0.16, 0.18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de4c678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the tree's hyperparameter grid\n",
    "# Define params_dt\n",
    "params_dt = {\n",
    "'max_depth': [2,3, 4],\n",
    "'min_samples_leaf': [0.12, 0.14, 0.16, 0.18] }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc3614",
   "metadata": {},
   "source": [
    "## Search for the optimal tree\n",
    "\n",
    "In this exercise, you'll perform grid search using 5-fold cross validation to find dt's optimal hyperparameters. Note that because grid search is an exhaustive process, it may take a lot time to train the model. Here you'll only be instantiating the GridSearchCV object without fitting it to the training set. As discussed in the video, you can train such an object similar to any scikit-learn estimator by using the .fit() method:\n",
    "\n",
    "> grid_object.fit(X_train, y_train)\n",
    "\n",
    "An untuned classification tree dt as well as the dictionary params_dt that you defined in the previous exercise are available in your workspace.\n",
    "**Instructions**\n",
    "* Import GridSearchCV from sklearn.model_selection.\n",
    "\n",
    "* Instantiate a GridSearchCV object using 5-fold CV by setting the parameters:\n",
    "\n",
    "    * estimator to dt, param_grid to params_dt and\n",
    "\n",
    "    * scoring to 'roc_auc'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fa582c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=1), n_jobs=-1,\n",
       "             param_grid={'max_depth': [2, 3, 4],\n",
       "                         'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performing the grid search.\n",
    "## Import GridSearchCV\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "## Instantiate grid_dt\n",
    "grid_dt = GridSearchCV(estimator=dt,\n",
    "                       param_grid=params_dt,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=5,\n",
    "                       n_jobs=-1)\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "#compute the test set ROC AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30657f15",
   "metadata": {},
   "source": [
    "## Evaluate the optimal tree\n",
    "\n",
    "In this exercise, you'll evaluate the test set ROC AUC score of grid_dt's optimal model.\n",
    "\n",
    "In order to do so, you will first determine the probability of obtaining the positive label for each test set observation. You can use the methodpredict_proba() of an sklearn classifier to compute a 2D array containing the probabilities of the negative and positive class-labels respectively along columns.\n",
    "\n",
    "The dataset is already loaded and processed for you (numerical features are standardized); it is split into 80% train and 20% test. X_test, y_test are available in your workspace. In addition, we have also loaded the trained GridSearchCV object grid_dt that you instantiated in the previous exercise. Note that grid_dt was trained as follows:\n",
    "\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "**Instructions**\n",
    "* Import roc_auc_score from sklearn.metrics.\n",
    "* Extract the .best_estimator_ attribute from grid_dt and assign it to best_model.\n",
    "* Predict the test set probabilities of obtaining the positive class y_pred_proba.\n",
    "* Compute the test set ROC AUC score test_roc_auc of best_model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbfa2cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ROC AUC score: 0.731\n"
     ]
    }
   ],
   "source": [
    "# Import roc_auc_score from sklearn.metrics \n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_dt.best_estimator_\n",
    "\n",
    "# Predict the test set probabilities of the positive class\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute test_roc_auc\n",
    "test_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Print test_roc_auc\n",
    "print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13c106",
   "metadata": {},
   "source": [
    "## Tuning an RF's Hyperparameters\n",
    "#### Random Forests Hyperparameters\n",
    "\n",
    "* CART hyperparameters\n",
    "* number of estimators\n",
    "* bootstrap #### Tuning is expensive\n",
    "* Hyperparameter tuning:\n",
    "    * computationally expensive,\n",
    "    * sometimes leads to very slight improvement,\n",
    "* Weight the impact of tuning on the whole project\n",
    "\n",
    "#### Doing:\n",
    "\n",
    "* Instantiate RF\n",
    "* Instantiate GridsearchCV\n",
    "* Evaluating the test set RMSE of the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fad9be",
   "metadata": {},
   "source": [
    "## Set the hyperparameter grid of RF\n",
    "\n",
    "In this exercise, you'll manually set the grid of hyperparameters that will be used to tune rf's hyperparameters and find the optimal regressor. For this purpose, you will be constructing a grid of hyperparameters and tune the number of estimators, the maximum number of features used when splitting each node and the minimum number of samples (or fraction) per leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "891c208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary 'params_rf'\n",
    "params_rf = {\n",
    "    'n_estimators': [100, 350, 500],\n",
    "    'max_features':['log2', 'auto', 'sqrt'],\n",
    "    'min_samples_leaf':[2, 10, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caa22a",
   "metadata": {},
   "source": [
    "## Search for the optimal forest\n",
    "\n",
    "In this exercise, you'll perform grid search using 3-fold cross validation to find rf's optimal hyperparameters. To evaluate each model in the grid, you'll be using the negative mean squared error metric.\n",
    "\n",
    "Note that because grid search is an exhaustive search process, it may take a lot time to train the model. Here you'll only be instantiating the GridSearchCV object without fitting it to the training set. As discussed in the video, you can train such an object similar to any scikit-learn estimator by using the .fit() method:\n",
    "\n",
    "> grid_object.fit(X_train, y_train)\n",
    "\n",
    "The untuned random forests regressor model rf as well as the dictionary params_rf that you defined in the previous exercise are available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63f5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GridSearchCV\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate grid_rf\n",
    "grid_rf = GridSearchCV(estimator=rf,\n",
    "                       param_grid=params_rf,\n",
    "                       scoring='neg_mean_squared_error',\n",
    "                       cv=3,\n",
    "                       verbose=1,\n",
    "                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a9c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestRegressor(), n_jobs=-1,\n",
       "             param_grid={'max_features': ['log2', 'auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [2, 10, 30],\n",
       "                         'n_estimators': [100, 350, 500]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike = pd.read_csv('../bikes.csv')\n",
    "X = bike[['hr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'instant',\n",
    "       'mnth', 'yr', 'Clear to partly cloudy', 'Light Precipitation', 'Misty']]\n",
    "y = bike['cnt']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# Fit with train set\n",
    "grid_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78e1d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE of best model: 51.421\n"
     ]
    }
   ],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE \n",
    "# from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Extract the best estimator\n",
    "best_model = grid_rf.best_estimator_\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute rmse_test\n",
    "rmse_test = MSE(y_test,y_pred)**(1/2)\n",
    "\n",
    "# Print rmse_test\n",
    "print('Test RMSE of best model: {:.3f}'.format(rmse_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb4502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0b701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c744d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6b15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882e2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960db629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
